defaults:
  - _self_
  - dataset
  - model

epoch: 1

lr: 5e-4

# sometimes we want to finetune backbone with a smaller learning rate
# set to null or 0 to freeze backbone
pe_lr: 1e-5

weight_decay: 1e-3
warmup_step: 0.1
decay_step: 0.1
batch_size: 48
strategy: ddp # Options: "deepspeed_stage_2_offload" / "ddp"
accumulate_grad_batches: 8
grad_clip_val: 1.0
precision: 16-mixed

cls_loss_weight: 1.0
scheduler: "wsd" # Options: "cosine" / "wsd"
