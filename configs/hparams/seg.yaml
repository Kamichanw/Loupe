defaults:
  - default
  - _self_

lr: 3e-4
weight_decay: 5e-2

# sometimes we want to finetune backbone with a smaller learning rate
# set to null to freeze backbone
backbone_lr: null

epoch: 5
batch_size: 40
accumulate_grad_batches: 3
