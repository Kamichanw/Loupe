defaults:
  - base
  - _self_

weight_decay: 5e-2

seg_lr: null # lr for seg head
backbone_lr: 1e-5 # sometimes we want to finetune backbone with a smaller learning rate
lr: 3e-4 # default lr for other not specified params

epoch: 1
batch_size: 40
accumulate_grad_batches: 3
